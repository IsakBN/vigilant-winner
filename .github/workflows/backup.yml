# =============================================================================
# BundleNudge 3-Tier Backup System
# =============================================================================
#
# This workflow implements a comprehensive backup strategy:
#   Tier 1: Railway S3 (hourly D1/Postgres, daily R2 sync)
#   Tier 2: Backblaze B2 (weekly archive)
#
# Schedule:
#   - Hourly: D1 and Postgres backups to Railway S3
#   - Daily (2 AM UTC): R2 bundles sync to Railway S3
#   - Weekly (Sundays 3 AM UTC): Archive to Backblaze B2
#
# Retention:
#   - Hourly backups: 30 days (720 backups)
#   - Weekly archives: 1 year (52 archives)
#
# =============================================================================
# REQUIRED SECRETS:
# =============================================================================
# Cloudflare:
#   - CLOUDFLARE_API_TOKEN: API token with D1 read access
#   - CLOUDFLARE_ACCOUNT_ID: Cloudflare account ID
#
# Railway S3 (Primary backup destination):
#   - RAILWAY_S3_ENDPOINT: Railway S3-compatible endpoint URL
#   - RAILWAY_S3_ACCESS_KEY: Railway S3 access key
#   - RAILWAY_S3_SECRET_KEY: Railway S3 secret key
#   - RAILWAY_S3_BUCKET: Railway S3 bucket name (e.g., "bundlenudge-backups")
#
# Railway Postgres (both databases):
#   - RAILWAY_POSTGRES_URL_AUTH: Auth database connection string
#   - RAILWAY_POSTGRES_URL_TELEMETRY: Telemetry database connection string
#
# Cloudflare R2:
#   - R2_ACCESS_KEY_ID: R2 access key ID
#   - R2_SECRET_ACCESS_KEY: R2 secret access key
#   - R2_ENDPOINT: R2 endpoint URL (e.g., https://<account>.r2.cloudflarestorage.com)
#   - R2_BUCKET: R2 bucket name for bundles
#
# Backblaze B2 (Archive destination):
#   - B2_APPLICATION_KEY_ID: B2 application key ID
#   - B2_APPLICATION_KEY: B2 application key
#   - B2_BUCKET: B2 bucket name for archives
#
# Notifications (optional):
#   - SLACK_WEBHOOK_URL: Slack webhook for success/failure notifications
# =============================================================================

name: Backup System

on:
  schedule:
    # Hourly: D1 and Postgres backups
    - cron: '0 * * * *'
    # Daily at 2 AM UTC: R2 bundles sync
    - cron: '0 2 * * *'
    # Weekly on Sundays at 3 AM UTC: Archive to B2
    - cron: '0 3 * * 0'

  # Allow manual triggers with options
  workflow_dispatch:
    inputs:
      backup_type:
        description: 'Type of backup to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - d1
          - postgres
          - r2-sync
          - archive-b2
      force_cleanup:
        description: 'Force cleanup of old backups'
        required: false
        default: false
        type: boolean

# Permissions for creating issues on failure
permissions:
  contents: read
  issues: write

# Prevent concurrent backup runs
concurrency:
  group: backup-system
  cancel-in-progress: false

env:
  BACKUP_RETENTION_HOURS: 720  # 30 days of hourly backups
  ARCHIVE_RETENTION_WEEKS: 52  # 1 year of weekly archives

jobs:
  # ===========================================================================
  # Job: Backup Cloudflare D1 Database
  # ===========================================================================
  backup-d1:
    name: Backup D1 Database
    runs-on: ubuntu-latest
    timeout-minutes: 30
    # Run hourly or on manual trigger
    if: >
      github.event_name == 'workflow_dispatch' &&
      (github.event.inputs.backup_type == 'all' || github.event.inputs.backup_type == 'd1')
      || (github.event_name == 'schedule' && github.event.schedule == '0 * * * *')

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install wrangler
        run: npm install -g wrangler@latest

      - name: Generate timestamp
        id: timestamp
        run: |
          echo "timestamp=$(date -u +%Y%m%d_%H%M%S)" >> $GITHUB_OUTPUT
          echo "date=$(date -u +%Y-%m-%d)" >> $GITHUB_OUTPUT

      - name: Export D1 database
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          set -euo pipefail

          echo "::group::Exporting D1 database"

          # Create backup directory
          mkdir -p backups/d1

          # Export D1 database to SQL
          wrangler d1 export bundlenudge-prod-db \
            --output=backups/d1/d1_backup_${{ steps.timestamp.outputs.timestamp }}.sql \
            --remote

          # Compress the backup
          gzip backups/d1/d1_backup_${{ steps.timestamp.outputs.timestamp }}.sql

          # Calculate checksum
          sha256sum backups/d1/d1_backup_${{ steps.timestamp.outputs.timestamp }}.sql.gz > \
            backups/d1/d1_backup_${{ steps.timestamp.outputs.timestamp }}.sql.gz.sha256

          # Log backup size
          ls -lh backups/d1/

          echo "::endgroup::"

      - name: Upload D1 backup to Railway S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set -euo pipefail

          echo "::group::Uploading D1 backup to Railway S3"

          # Install AWS CLI
          pip install awscli --quiet

          # Upload backup and checksum
          aws s3 cp backups/d1/d1_backup_${{ steps.timestamp.outputs.timestamp }}.sql.gz \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/d1/hourly/${{ steps.timestamp.outputs.date }}/ \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          aws s3 cp backups/d1/d1_backup_${{ steps.timestamp.outputs.timestamp }}.sql.gz.sha256 \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/d1/hourly/${{ steps.timestamp.outputs.date }}/ \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          echo "D1 backup uploaded successfully"
          echo "::endgroup::"

      - name: Verify D1 backup integrity
        id: verify
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set -euo pipefail

          echo "::group::Verifying D1 backup"

          # Download backup and checksum file
          aws s3 cp \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/d1/hourly/${{ steps.timestamp.outputs.date }}/d1_backup_${{ steps.timestamp.outputs.timestamp }}.sql.gz \
            /tmp/verify_d1.sql.gz \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          aws s3 cp \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/d1/hourly/${{ steps.timestamp.outputs.date }}/d1_backup_${{ steps.timestamp.outputs.timestamp }}.sql.gz.sha256 \
            /tmp/original_checksum.txt \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          # Extract expected hash and compare with actual
          EXPECTED_HASH=$(awk '{print $1}' /tmp/original_checksum.txt)
          ACTUAL_HASH=$(sha256sum /tmp/verify_d1.sql.gz | awk '{print $1}')

          if [ "$EXPECTED_HASH" = "$ACTUAL_HASH" ]; then
            echo "D1 backup verification passed"
          else
            echo "Checksum mismatch! Expected: $EXPECTED_HASH, Got: $ACTUAL_HASH"
            exit 1
          fi

          # Get backup size in human-readable format
          BACKUP_SIZE=$(ls -lh /tmp/verify_d1.sql.gz | awk '{print $5}')
          echo "backup_size=$BACKUP_SIZE" >> $GITHUB_OUTPUT

          echo "::endgroup::"

      - name: Calculate duration
        id: duration
        run: |
          END_TIME=$(date +%s)
          # Approximate start time (workflow started ~2 minutes before this step)
          DURATION_SECONDS=$((END_TIME - $(date -d "${{ steps.timestamp.outputs.timestamp }}" +%s 2>/dev/null || echo $((END_TIME - 120)))))
          DURATION_MINUTES=$((DURATION_SECONDS / 60))
          DURATION_REMAINDER=$((DURATION_SECONDS % 60))
          echo "duration=${DURATION_MINUTES}m ${DURATION_REMAINDER}s" >> $GITHUB_OUTPUT

      - name: Send D1 backup success notification
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        if: env.SLACK_WEBHOOK_URL != ''
        run: |
          curl -X POST -H 'Content-type: application/json' \
            --data '{
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": ":white_check_mark: *D1 Database Backup Completed*"
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Size:*\n${{ steps.verify.outputs.backup_size }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Duration:*\n${{ steps.duration.outputs.duration }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*File:*\nd1_backup_${{ steps.timestamp.outputs.timestamp }}.sql.gz"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Date:*\n${{ steps.timestamp.outputs.date }}"
                    }
                  ]
                },
                {
                  "type": "context",
                  "elements": [
                    {
                      "type": "mrkdwn",
                      "text": "Destination: Railway S3 | <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>"
                    }
                  ]
                }
              ]
            }' \
            "$SLACK_WEBHOOK_URL"

    outputs:
      backup_file: d1_backup_${{ steps.timestamp.outputs.timestamp }}.sql.gz
      backup_date: ${{ steps.timestamp.outputs.date }}
      backup_size: ${{ steps.verify.outputs.backup_size }}
      duration: ${{ steps.duration.outputs.duration }}

  # ===========================================================================
  # Job: Backup Railway Postgres Databases (Auth + Telemetry)
  # ===========================================================================
  backup-postgres:
    name: Backup Postgres Databases
    runs-on: ubuntu-latest
    timeout-minutes: 60
    # Run hourly or on manual trigger
    if: >
      github.event_name == 'workflow_dispatch' &&
      (github.event.inputs.backup_type == 'all' || github.event.inputs.backup_type == 'postgres')
      || (github.event_name == 'schedule' && github.event.schedule == '0 * * * *')

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install PostgreSQL 17 client
        run: |
          # Add PostgreSQL APT repository
          sudo apt-get install -y curl ca-certificates gnupg
          sudo install -d /usr/share/postgresql-common/pgdg
          sudo curl -o /usr/share/postgresql-common/pgdg/apt.postgresql.org.asc --fail https://www.postgresql.org/media/keys/ACCC4CF8.asc
          sudo sh -c 'echo "deb [signed-by=/usr/share/postgresql-common/pgdg/apt.postgresql.org.asc] https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          sudo apt-get update

          # Remove old postgresql-client if present, install v17
          sudo apt-get remove -y postgresql-client postgresql-client-16 postgresql-client-common || true
          sudo apt-get install -y postgresql-client-17

          # Verify correct version
          /usr/lib/postgresql/17/bin/pg_dump --version

          # Add to PATH for this job
          echo "/usr/lib/postgresql/17/bin" >> $GITHUB_PATH

      - name: Install AWS CLI
        run: pip install awscli --quiet

      - name: Generate timestamp
        id: timestamp
        run: |
          echo "timestamp=$(date -u +%Y%m%d_%H%M%S)" >> $GITHUB_OUTPUT
          echo "date=$(date -u +%Y-%m-%d)" >> $GITHUB_OUTPUT

      - name: Export Auth database
        env:
          DATABASE_URL: ${{ secrets.RAILWAY_POSTGRES_URL_AUTH }}
        run: |
          set -euo pipefail

          echo "::group::Exporting Auth Postgres database"

          mkdir -p backups/postgres-auth

          pg_dump "$DATABASE_URL" \
            --format=custom \
            --compress=9 \
            --no-owner \
            --no-privileges \
            --clean \
            --if-exists \
            --file=backups/postgres-auth/auth_backup_${{ steps.timestamp.outputs.timestamp }}.dump

          sha256sum backups/postgres-auth/auth_backup_${{ steps.timestamp.outputs.timestamp }}.dump > \
            backups/postgres-auth/auth_backup_${{ steps.timestamp.outputs.timestamp }}.dump.sha256

          ls -lh backups/postgres-auth/

          echo "::endgroup::"

      - name: Export Telemetry database
        env:
          DATABASE_URL: ${{ secrets.RAILWAY_POSTGRES_URL_TELEMETRY }}
        run: |
          set -euo pipefail

          echo "::group::Exporting Telemetry Postgres database"

          mkdir -p backups/postgres-telemetry

          pg_dump "$DATABASE_URL" \
            --format=custom \
            --compress=9 \
            --no-owner \
            --no-privileges \
            --clean \
            --if-exists \
            --file=backups/postgres-telemetry/telemetry_backup_${{ steps.timestamp.outputs.timestamp }}.dump

          sha256sum backups/postgres-telemetry/telemetry_backup_${{ steps.timestamp.outputs.timestamp }}.dump > \
            backups/postgres-telemetry/telemetry_backup_${{ steps.timestamp.outputs.timestamp }}.dump.sha256

          ls -lh backups/postgres-telemetry/

          echo "::endgroup::"

      - name: Upload Auth backup to Railway S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set -euo pipefail

          echo "::group::Uploading Auth backup to Railway S3"

          aws s3 cp backups/postgres-auth/auth_backup_${{ steps.timestamp.outputs.timestamp }}.dump \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-auth/hourly/${{ steps.timestamp.outputs.date }}/ \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          aws s3 cp backups/postgres-auth/auth_backup_${{ steps.timestamp.outputs.timestamp }}.dump.sha256 \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-auth/hourly/${{ steps.timestamp.outputs.date }}/ \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          echo "Auth backup uploaded successfully"
          echo "::endgroup::"

      - name: Upload Telemetry backup to Railway S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set -euo pipefail

          echo "::group::Uploading Telemetry backup to Railway S3"

          aws s3 cp backups/postgres-telemetry/telemetry_backup_${{ steps.timestamp.outputs.timestamp }}.dump \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-telemetry/hourly/${{ steps.timestamp.outputs.date }}/ \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          aws s3 cp backups/postgres-telemetry/telemetry_backup_${{ steps.timestamp.outputs.timestamp }}.dump.sha256 \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-telemetry/hourly/${{ steps.timestamp.outputs.date }}/ \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          echo "Telemetry backup uploaded successfully"
          echo "::endgroup::"

      - name: Verify Auth backup integrity
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set -euo pipefail

          echo "::group::Verifying Auth backup"

          aws s3 cp \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-auth/hourly/${{ steps.timestamp.outputs.date }}/auth_backup_${{ steps.timestamp.outputs.timestamp }}.dump \
            /tmp/verify_auth.dump \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          aws s3 cp \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-auth/hourly/${{ steps.timestamp.outputs.date }}/auth_backup_${{ steps.timestamp.outputs.timestamp }}.dump.sha256 \
            /tmp/auth_checksum.txt \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          # Verify checksum
          EXPECTED_HASH=$(awk '{print $1}' /tmp/auth_checksum.txt)
          ACTUAL_HASH=$(sha256sum /tmp/verify_auth.dump | awk '{print $1}')
          if [ "$EXPECTED_HASH" != "$ACTUAL_HASH" ]; then
            echo "Auth checksum mismatch!"; exit 1
          fi

          # Verify dump is valid
          pg_restore --list /tmp/verify_auth.dump > /dev/null

          echo "Auth backup verification passed"
          echo "::endgroup::"

      - name: Verify Telemetry backup integrity
        id: verify_telemetry
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set -euo pipefail

          echo "::group::Verifying Telemetry backup"

          aws s3 cp \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-telemetry/hourly/${{ steps.timestamp.outputs.date }}/telemetry_backup_${{ steps.timestamp.outputs.timestamp }}.dump \
            /tmp/verify_telemetry.dump \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          aws s3 cp \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-telemetry/hourly/${{ steps.timestamp.outputs.date }}/telemetry_backup_${{ steps.timestamp.outputs.timestamp }}.dump.sha256 \
            /tmp/telemetry_checksum.txt \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

          # Verify checksum
          EXPECTED_HASH=$(awk '{print $1}' /tmp/telemetry_checksum.txt)
          ACTUAL_HASH=$(sha256sum /tmp/verify_telemetry.dump | awk '{print $1}')
          if [ "$EXPECTED_HASH" != "$ACTUAL_HASH" ]; then
            echo "Telemetry checksum mismatch!"; exit 1
          fi

          # Verify dump is valid
          pg_restore --list /tmp/verify_telemetry.dump > /dev/null

          # Get backup size
          TELEMETRY_SIZE=$(ls -lh /tmp/verify_telemetry.dump | awk '{print $5}')
          echo "telemetry_size=$TELEMETRY_SIZE" >> $GITHUB_OUTPUT

          echo "Telemetry backup verification passed"
          echo "::endgroup::"

      - name: Get Auth backup size
        id: verify_auth
        run: |
          AUTH_SIZE=$(ls -lh backups/postgres-auth/auth_backup_${{ steps.timestamp.outputs.timestamp }}.dump | awk '{print $5}')
          echo "auth_size=$AUTH_SIZE" >> $GITHUB_OUTPUT

      - name: Calculate duration
        id: duration
        run: |
          END_TIME=$(date +%s)
          DURATION_SECONDS=$((END_TIME - $(date -d "${{ steps.timestamp.outputs.timestamp }}" +%s 2>/dev/null || echo $((END_TIME - 300)))))
          DURATION_MINUTES=$((DURATION_SECONDS / 60))
          DURATION_REMAINDER=$((DURATION_SECONDS % 60))
          echo "duration=${DURATION_MINUTES}m ${DURATION_REMAINDER}s" >> $GITHUB_OUTPUT

      - name: Send Auth Postgres backup success notification
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        if: env.SLACK_WEBHOOK_URL != ''
        run: |
          curl -X POST -H 'Content-type: application/json' \
            --data '{
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": ":white_check_mark: *Auth Postgres Backup Completed*"
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Size:*\n${{ steps.verify_auth.outputs.auth_size }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*File:*\nauth_backup_${{ steps.timestamp.outputs.timestamp }}.dump"
                    }
                  ]
                },
                {
                  "type": "context",
                  "elements": [
                    {
                      "type": "mrkdwn",
                      "text": "Destination: Railway S3 | <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>"
                    }
                  ]
                }
              ]
            }' \
            "$SLACK_WEBHOOK_URL"

      - name: Send Telemetry Postgres backup success notification
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        if: env.SLACK_WEBHOOK_URL != ''
        run: |
          curl -X POST -H 'Content-type: application/json' \
            --data '{
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": ":white_check_mark: *Telemetry Postgres Backup Completed*"
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Size:*\n${{ steps.verify_telemetry.outputs.telemetry_size }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Duration (total):*\n${{ steps.duration.outputs.duration }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*File:*\ntelemetry_backup_${{ steps.timestamp.outputs.timestamp }}.dump"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Date:*\n${{ steps.timestamp.outputs.date }}"
                    }
                  ]
                },
                {
                  "type": "context",
                  "elements": [
                    {
                      "type": "mrkdwn",
                      "text": "Destination: Railway S3 | <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>"
                    }
                  ]
                }
              ]
            }' \
            "$SLACK_WEBHOOK_URL"

    outputs:
      auth_backup_file: auth_backup_${{ steps.timestamp.outputs.timestamp }}.dump
      telemetry_backup_file: telemetry_backup_${{ steps.timestamp.outputs.timestamp }}.dump
      backup_date: ${{ steps.timestamp.outputs.date }}
      auth_size: ${{ steps.verify_auth.outputs.auth_size }}
      telemetry_size: ${{ steps.verify_telemetry.outputs.telemetry_size }}
      duration: ${{ steps.duration.outputs.duration }}

  # ===========================================================================
  # Job: Sync R2 Bundles to Railway S3
  # ===========================================================================
  sync-bundles:
    name: Sync R2 Bundles
    runs-on: ubuntu-latest
    timeout-minutes: 120
    # Run daily at 2 AM UTC or on manual trigger
    if: >
      github.event_name == 'workflow_dispatch' &&
      (github.event.inputs.backup_type == 'all' || github.event.inputs.backup_type == 'r2-sync')
      || (github.event_name == 'schedule' && github.event.schedule == '0 2 * * *')

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install rclone
        run: |
          curl https://rclone.org/install.sh | sudo bash

      - name: Configure rclone
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf << EOF
          [r2]
          type = s3
          provider = Cloudflare
          access_key_id = ${{ secrets.R2_ACCESS_KEY_ID }}
          secret_access_key = ${{ secrets.R2_SECRET_ACCESS_KEY }}
          endpoint = ${{ secrets.R2_ENDPOINT }}
          acl = private

          [railway]
          type = s3
          provider = Other
          access_key_id = ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          secret_access_key = ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          endpoint = ${{ secrets.RAILWAY_S3_ENDPOINT }}
          acl = private
          EOF

      - name: Generate timestamp
        id: timestamp
        run: |
          echo "timestamp=$(date -u +%Y%m%d_%H%M%S)" >> $GITHUB_OUTPUT
          echo "date=$(date -u +%Y-%m-%d)" >> $GITHUB_OUTPUT

      - name: Sync bundles from R2 to Railway S3
        run: |
          set -euo pipefail

          echo "::group::Syncing R2 bundles to Railway S3"

          # Sync with checksums for integrity verification
          # --checksum: Compare checksums instead of modification time
          # --transfers: Number of parallel transfers
          # --checkers: Number of parallel checkers
          # --stats: Show transfer stats every 30 seconds
          # --log-level: Set log level to INFO
          rclone sync r2:${{ secrets.R2_BUCKET }} \
            railway:${{ secrets.RAILWAY_S3_BUCKET }}/bundles/ \
            --checksum \
            --transfers 8 \
            --checkers 16 \
            --stats 30s \
            --log-level INFO \
            --stats-one-line

          echo "::endgroup::"

      - name: Verify sync integrity
        run: |
          set -euo pipefail

          echo "::group::Verifying sync integrity"

          # Count objects in source and destination
          SOURCE_COUNT=$(rclone size r2:${{ secrets.R2_BUCKET }} --json | jq '.count')
          DEST_COUNT=$(rclone size railway:${{ secrets.RAILWAY_S3_BUCKET }}/bundles/ --json | jq '.count')

          echo "Source objects: $SOURCE_COUNT"
          echo "Destination objects: $DEST_COUNT"

          # Verify counts match
          if [ "$SOURCE_COUNT" != "$DEST_COUNT" ]; then
            echo "::error::Object count mismatch! Source: $SOURCE_COUNT, Dest: $DEST_COUNT"
            exit 1
          fi

          # Run check to verify checksums
          rclone check r2:${{ secrets.R2_BUCKET }} \
            railway:${{ secrets.RAILWAY_S3_BUCKET }}/bundles/ \
            --one-way \
            --size-only

          echo "Bundle sync verification passed"
          echo "::endgroup::"

      - name: Get sync stats
        id: sync_stats
        run: |
          set -euo pipefail

          # Get object count and size
          SYNC_INFO=$(rclone size railway:${{ secrets.RAILWAY_S3_BUCKET }}/bundles/ --json 2>/dev/null || echo '{"bytes":0,"count":0}')
          OBJECT_COUNT=$(echo "$SYNC_INFO" | jq '.count')
          TOTAL_BYTES=$(echo "$SYNC_INFO" | jq '.bytes')

          # Convert to human-readable
          if [ "$TOTAL_BYTES" -gt 1073741824 ]; then
            HUMAN_SIZE="$(echo "scale=2; $TOTAL_BYTES / 1073741824" | bc)GB"
          elif [ "$TOTAL_BYTES" -gt 1048576 ]; then
            HUMAN_SIZE="$(echo "scale=2; $TOTAL_BYTES / 1048576" | bc)MB"
          elif [ "$TOTAL_BYTES" -gt 1024 ]; then
            HUMAN_SIZE="$(echo "scale=2; $TOTAL_BYTES / 1024" | bc)KB"
          else
            HUMAN_SIZE="${TOTAL_BYTES}B"
          fi

          echo "object_count=$OBJECT_COUNT" >> $GITHUB_OUTPUT
          echo "total_size=$HUMAN_SIZE" >> $GITHUB_OUTPUT

      - name: Create sync manifest
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set -euo pipefail

          # Install AWS CLI
          pip install awscli --quiet

          # Create manifest with sync metadata
          cat > /tmp/sync_manifest.json << EOF
          {
            "sync_timestamp": "${{ steps.timestamp.outputs.timestamp }}",
            "sync_date": "${{ steps.timestamp.outputs.date }}",
            "source": "r2:${{ secrets.R2_BUCKET }}",
            "destination": "railway:${{ secrets.RAILWAY_S3_BUCKET }}/bundles/",
            "github_run_id": "${{ github.run_id }}",
            "github_run_number": "${{ github.run_number }}"
          }
          EOF

          aws s3 cp /tmp/sync_manifest.json \
            s3://${{ secrets.RAILWAY_S3_BUCKET }}/bundles/_manifests/sync_${{ steps.timestamp.outputs.timestamp }}.json \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}

      - name: Send R2 sync success notification
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        if: env.SLACK_WEBHOOK_URL != ''
        run: |
          curl -X POST -H 'Content-type: application/json' \
            --data '{
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": ":white_check_mark: *R2 Bundles Sync Completed*"
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Objects Synced:*\n${{ steps.sync_stats.outputs.object_count }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Total Size:*\n${{ steps.sync_stats.outputs.total_size }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Source:*\nCloudflare R2"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Destination:*\nRailway S3"
                    }
                  ]
                },
                {
                  "type": "context",
                  "elements": [
                    {
                      "type": "mrkdwn",
                      "text": "Date: ${{ steps.timestamp.outputs.date }} | <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>"
                    }
                  ]
                }
              ]
            }' \
            "$SLACK_WEBHOOK_URL"

    outputs:
      sync_date: ${{ steps.timestamp.outputs.date }}
      object_count: ${{ steps.sync_stats.outputs.object_count }}
      total_size: ${{ steps.sync_stats.outputs.total_size }}

  # ===========================================================================
  # Job: Archive to Backblaze B2 (Weekly)
  # ===========================================================================
  archive-to-b2:
    name: Archive to Backblaze B2
    runs-on: ubuntu-latest
    timeout-minutes: 180
    # Run weekly on Sundays at 3 AM UTC or on manual trigger
    if: >
      github.event_name == 'workflow_dispatch' &&
      (github.event.inputs.backup_type == 'all' || github.event.inputs.backup_type == 'archive-b2')
      || (github.event_name == 'schedule' && github.event.schedule == '0 3 * * 0')

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install rclone
        run: |
          curl https://rclone.org/install.sh | sudo bash

      - name: Configure rclone
        run: |
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf << EOF
          [railway]
          type = s3
          provider = Other
          access_key_id = ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          secret_access_key = ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          endpoint = ${{ secrets.RAILWAY_S3_ENDPOINT }}
          acl = private

          [b2]
          type = b2
          account = ${{ secrets.B2_APPLICATION_KEY_ID }}
          key = ${{ secrets.B2_APPLICATION_KEY }}
          hard_delete = true
          EOF

      - name: Generate timestamp
        id: timestamp
        run: |
          echo "timestamp=$(date -u +%Y%m%d_%H%M%S)" >> $GITHUB_OUTPUT
          echo "week=$(date -u +%Y-W%V)" >> $GITHUB_OUTPUT
          echo "year=$(date -u +%Y)" >> $GITHUB_OUTPUT

      - name: Archive D1 backups to B2
        run: |
          set -euo pipefail

          echo "::group::Archiving D1 backups to B2"

          # Copy all D1 backups from the past week
          rclone copy railway:${{ secrets.RAILWAY_S3_BUCKET }}/d1/hourly/ \
            b2:${{ secrets.B2_BUCKET }}/archives/${{ steps.timestamp.outputs.year }}/${{ steps.timestamp.outputs.week }}/d1/ \
            --transfers 8 \
            --checkers 16 \
            --stats 30s \
            --log-level INFO

          echo "::endgroup::"

      - name: Archive Auth Postgres backups to B2
        run: |
          set -euo pipefail

          echo "::group::Archiving Auth Postgres backups to B2"

          rclone copy railway:${{ secrets.RAILWAY_S3_BUCKET }}/postgres-auth/hourly/ \
            b2:${{ secrets.B2_BUCKET }}/archives/${{ steps.timestamp.outputs.year }}/${{ steps.timestamp.outputs.week }}/postgres-auth/ \
            --transfers 4 \
            --checkers 8 \
            --stats 30s \
            --log-level INFO

          echo "::endgroup::"

      - name: Archive Telemetry Postgres backups to B2
        run: |
          set -euo pipefail

          echo "::group::Archiving Telemetry Postgres backups to B2"

          rclone copy railway:${{ secrets.RAILWAY_S3_BUCKET }}/postgres-telemetry/hourly/ \
            b2:${{ secrets.B2_BUCKET }}/archives/${{ steps.timestamp.outputs.year }}/${{ steps.timestamp.outputs.week }}/postgres-telemetry/ \
            --transfers 4 \
            --checkers 8 \
            --stats 30s \
            --log-level INFO

          echo "::endgroup::"

      - name: Archive bundles to B2
        run: |
          set -euo pipefail

          echo "::group::Archiving bundles to B2"

          # Sync bundles to B2 (incremental)
          rclone sync railway:${{ secrets.RAILWAY_S3_BUCKET }}/bundles/ \
            b2:${{ secrets.B2_BUCKET }}/bundles/ \
            --checksum \
            --transfers 8 \
            --checkers 16 \
            --stats 30s \
            --log-level INFO

          echo "::endgroup::"

      - name: Create archive manifest and get stats
        id: archive_stats
        run: |
          set -euo pipefail

          # Get archive stats
          D1_SIZE=$(rclone size b2:${{ secrets.B2_BUCKET }}/archives/${{ steps.timestamp.outputs.year }}/${{ steps.timestamp.outputs.week }}/d1/ --json 2>/dev/null || echo '{"bytes":0,"count":0}')
          PG_AUTH_SIZE=$(rclone size b2:${{ secrets.B2_BUCKET }}/archives/${{ steps.timestamp.outputs.year }}/${{ steps.timestamp.outputs.week }}/postgres-auth/ --json 2>/dev/null || echo '{"bytes":0,"count":0}')
          PG_TELEMETRY_SIZE=$(rclone size b2:${{ secrets.B2_BUCKET }}/archives/${{ steps.timestamp.outputs.year }}/${{ steps.timestamp.outputs.week }}/postgres-telemetry/ --json 2>/dev/null || echo '{"bytes":0,"count":0}')
          BUNDLE_SIZE=$(rclone size b2:${{ secrets.B2_BUCKET }}/bundles/ --json 2>/dev/null || echo '{"bytes":0,"count":0}')

          # Calculate total bytes
          D1_BYTES=$(echo "$D1_SIZE" | jq '.bytes')
          AUTH_BYTES=$(echo "$PG_AUTH_SIZE" | jq '.bytes')
          TELEMETRY_BYTES=$(echo "$PG_TELEMETRY_SIZE" | jq '.bytes')
          BUNDLE_BYTES=$(echo "$BUNDLE_SIZE" | jq '.bytes')
          TOTAL_BYTES=$((D1_BYTES + AUTH_BYTES + TELEMETRY_BYTES + BUNDLE_BYTES))

          # Convert to human-readable
          format_size() {
            local bytes=$1
            if [ "$bytes" -gt 1073741824 ]; then
              echo "$(echo "scale=2; $bytes / 1073741824" | bc)GB"
            elif [ "$bytes" -gt 1048576 ]; then
              echo "$(echo "scale=2; $bytes / 1048576" | bc)MB"
            elif [ "$bytes" -gt 1024 ]; then
              echo "$(echo "scale=2; $bytes / 1024" | bc)KB"
            else
              echo "${bytes}B"
            fi
          }

          TOTAL_HUMAN=$(format_size $TOTAL_BYTES)
          D1_HUMAN=$(format_size $D1_BYTES)
          AUTH_HUMAN=$(format_size $AUTH_BYTES)
          TELEMETRY_HUMAN=$(format_size $TELEMETRY_BYTES)
          BUNDLE_HUMAN=$(format_size $BUNDLE_BYTES)

          echo "total_size=$TOTAL_HUMAN" >> $GITHUB_OUTPUT
          echo "d1_size=$D1_HUMAN" >> $GITHUB_OUTPUT
          echo "auth_size=$AUTH_HUMAN" >> $GITHUB_OUTPUT
          echo "telemetry_size=$TELEMETRY_HUMAN" >> $GITHUB_OUTPUT
          echo "bundle_size=$BUNDLE_HUMAN" >> $GITHUB_OUTPUT

          # Create manifest
          cat > /tmp/archive_manifest.json << EOF
          {
            "archive_timestamp": "${{ steps.timestamp.outputs.timestamp }}",
            "archive_week": "${{ steps.timestamp.outputs.week }}",
            "d1_archive": $D1_SIZE,
            "postgres_auth_archive": $PG_AUTH_SIZE,
            "postgres_telemetry_archive": $PG_TELEMETRY_SIZE,
            "bundles_total": $BUNDLE_SIZE,
            "github_run_id": "${{ github.run_id }}",
            "github_run_number": "${{ github.run_number }}"
          }
          EOF

          # Upload manifest to B2
          rclone copyto /tmp/archive_manifest.json \
            b2:${{ secrets.B2_BUCKET }}/archives/${{ steps.timestamp.outputs.year }}/${{ steps.timestamp.outputs.week }}/_manifest.json

      - name: Send B2 archive success notification
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        if: env.SLACK_WEBHOOK_URL != ''
        run: |
          curl -X POST -H 'Content-type: application/json' \
            --data '{
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": ":white_check_mark: *Backblaze B2 Archive Completed*"
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Total Archived:*\n${{ steps.archive_stats.outputs.total_size }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Archive Week:*\n${{ steps.timestamp.outputs.week }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*D1 Backups:*\n${{ steps.archive_stats.outputs.d1_size }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Auth Postgres:*\n${{ steps.archive_stats.outputs.auth_size }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Telemetry Postgres:*\n${{ steps.archive_stats.outputs.telemetry_size }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Bundles:*\n${{ steps.archive_stats.outputs.bundle_size }}"
                    }
                  ]
                },
                {
                  "type": "context",
                  "elements": [
                    {
                      "type": "mrkdwn",
                      "text": "Destination: Backblaze B2 | <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>"
                    }
                  ]
                }
              ]
            }' \
            "$SLACK_WEBHOOK_URL"

    outputs:
      archive_week: ${{ steps.timestamp.outputs.week }}
      total_size: ${{ steps.archive_stats.outputs.total_size }}

  # ===========================================================================
  # Job: Cleanup Old Backups
  # ===========================================================================
  cleanup:
    name: Cleanup Old Backups
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [backup-d1, backup-postgres]
    # Run after hourly backups complete, or on manual force cleanup
    if: >
      always() &&
      (needs.backup-d1.result == 'success' || needs.backup-postgres.result == 'success' ||
       (github.event_name == 'workflow_dispatch' && github.event.inputs.force_cleanup == 'true'))

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install AWS CLI
        run: pip install awscli --quiet

      - name: Calculate cutoff date
        id: cutoff
        run: |
          # Calculate date 30 days ago for hourly backups
          CUTOFF_DATE=$(date -u -d "30 days ago" +%Y-%m-%d 2>/dev/null || date -u -v-30d +%Y-%m-%d)
          echo "cutoff_date=$CUTOFF_DATE" >> $GITHUB_OUTPUT
          echo "Cleanup cutoff date: $CUTOFF_DATE"

      - name: Cleanup old D1 backups
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set -euo pipefail

          echo "::group::Cleaning up old D1 backups"

          # List all date directories
          DATES=$(aws s3 ls s3://${{ secrets.RAILWAY_S3_BUCKET }}/d1/hourly/ \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }} | awk '{print $2}' | tr -d '/')

          for DATE in $DATES; do
            if [[ "$DATE" < "${{ steps.cutoff.outputs.cutoff_date }}" ]]; then
              echo "Deleting D1 backups from $DATE"
              aws s3 rm s3://${{ secrets.RAILWAY_S3_BUCKET }}/d1/hourly/$DATE/ \
                --recursive \
                --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}
            fi
          done

          echo "::endgroup::"

      - name: Cleanup old Auth Postgres backups
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set -euo pipefail

          echo "::group::Cleaning up old Auth Postgres backups"

          DATES=$(aws s3 ls s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-auth/hourly/ \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }} 2>/dev/null | awk '{print $2}' | tr -d '/' || true)

          for DATE in $DATES; do
            if [[ -n "$DATE" && "$DATE" < "${{ steps.cutoff.outputs.cutoff_date }}" ]]; then
              echo "Deleting Auth Postgres backups from $DATE"
              aws s3 rm s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-auth/hourly/$DATE/ \
                --recursive \
                --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}
            fi
          done

          echo "::endgroup::"

      - name: Cleanup old Telemetry Postgres backups
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          set -euo pipefail

          echo "::group::Cleaning up old Telemetry Postgres backups"

          DATES=$(aws s3 ls s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-telemetry/hourly/ \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }} 2>/dev/null | awk '{print $2}' | tr -d '/' || true)

          for DATE in $DATES; do
            if [[ -n "$DATE" && "$DATE" < "${{ steps.cutoff.outputs.cutoff_date }}" ]]; then
              echo "Deleting Telemetry Postgres backups from $DATE"
              aws s3 rm s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-telemetry/hourly/$DATE/ \
                --recursive \
                --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }}
            fi
          done

          echo "::endgroup::"

      - name: Report cleanup results
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.RAILWAY_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.RAILWAY_S3_SECRET_KEY }}
          AWS_DEFAULT_REGION: us-east-1
        run: |
          echo "::group::Backup storage summary"

          echo "D1 backups:"
          aws s3 ls s3://${{ secrets.RAILWAY_S3_BUCKET }}/d1/hourly/ \
            --recursive --summarize \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }} 2>/dev/null | tail -2 || echo "  No backups yet"

          echo ""
          echo "Auth Postgres backups:"
          aws s3 ls s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-auth/hourly/ \
            --recursive --summarize \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }} 2>/dev/null | tail -2 || echo "  No backups yet"

          echo ""
          echo "Telemetry Postgres backups:"
          aws s3 ls s3://${{ secrets.RAILWAY_S3_BUCKET }}/postgres-telemetry/hourly/ \
            --recursive --summarize \
            --endpoint-url ${{ secrets.RAILWAY_S3_ENDPOINT }} 2>/dev/null | tail -2 || echo "  No backups yet"

          echo "::endgroup::"

  # ===========================================================================
  # Job: Notify on Failure
  # ===========================================================================
  notify-failure:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: [backup-d1, backup-postgres, sync-bundles, archive-to-b2, cleanup]
    if: failure()

    steps:
      - name: Determine failed jobs
        id: failed
        run: |
          FAILED_JOBS=""

          if [ "${{ needs.backup-d1.result }}" == "failure" ]; then
            FAILED_JOBS="${FAILED_JOBS}D1 Backup, "
          fi

          if [ "${{ needs.backup-postgres.result }}" == "failure" ]; then
            FAILED_JOBS="${FAILED_JOBS}Postgres Backup, "
          fi

          if [ "${{ needs.sync-bundles.result }}" == "failure" ]; then
            FAILED_JOBS="${FAILED_JOBS}R2 Sync, "
          fi

          if [ "${{ needs.archive-to-b2.result }}" == "failure" ]; then
            FAILED_JOBS="${FAILED_JOBS}B2 Archive, "
          fi

          if [ "${{ needs.cleanup.result }}" == "failure" ]; then
            FAILED_JOBS="${FAILED_JOBS}Cleanup, "
          fi

          # Remove trailing comma and space
          FAILED_JOBS="${FAILED_JOBS%, }"

          echo "failed_jobs=$FAILED_JOBS" >> $GITHUB_OUTPUT

      - name: Send Slack notification
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        if: env.SLACK_WEBHOOK_URL != ''
        run: |
          curl -X POST -H 'Content-type: application/json' \
            --data '{
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "Backup System Failure",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Failed Jobs:*\n${{ steps.failed.outputs.failed_jobs }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Run:*\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Details>"
                    }
                  ]
                },
                {
                  "type": "context",
                  "elements": [
                    {
                      "type": "mrkdwn",
                      "text": "Repository: ${{ github.repository }} | Workflow: ${{ github.workflow }}"
                    }
                  ]
                }
              ]
            }' \
            "$SLACK_WEBHOOK_URL"

      - name: Create GitHub Issue for failure
        uses: actions/github-script@v7
        with:
          script: |
            const failedJobs = '${{ steps.failed.outputs.failed_jobs }}';
            const runUrl = `${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`;

            // Check for existing open issue
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'backup-failure',
              state: 'open'
            });

            if (issues.data.length > 0) {
              // Add comment to existing issue
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issues.data[0].number,
                body: `### Additional Backup Failure\n\n**Failed Jobs:** ${failedJobs}\n**Run:** ${runUrl}\n**Time:** ${new Date().toISOString()}`
              });
            } else {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'Backup System Failure',
                labels: ['backup-failure', 'critical'],
                body: `## Backup System Failure\n\n**Failed Jobs:** ${failedJobs}\n\n**Run:** ${runUrl}\n\n**Time:** ${new Date().toISOString()}\n\n### Action Required\n\n1. Check the workflow run logs for detailed error messages\n2. Verify cloud provider credentials are valid\n3. Check storage quotas and permissions\n4. Re-run failed jobs manually if needed\n\n### Manual Trigger\n\nTo re-run specific backups:\n\`\`\`bash\ngh workflow run backup.yml -f backup_type=<d1|postgres|r2-sync|archive-b2>\n\`\`\``
              });
            }

  # ===========================================================================
  # Job: Daily Summary (After Hourly Backups)
  # ===========================================================================
  daily-summary:
    name: Daily Backup Summary
    runs-on: ubuntu-latest
    needs: [backup-d1, backup-postgres, cleanup]
    # Run after hourly backups complete successfully
    if: >
      always() &&
      (needs.backup-d1.result == 'success' || needs.backup-postgres.result == 'success') &&
      (github.event_name == 'schedule' && github.event.schedule == '0 * * * *')

    steps:
      - name: Determine job statuses
        id: status
        run: |
          # D1 status
          if [ "${{ needs.backup-d1.result }}" == "success" ]; then
            echo "d1_status=:white_check_mark: Success" >> $GITHUB_OUTPUT
          elif [ "${{ needs.backup-d1.result }}" == "skipped" ]; then
            echo "d1_status=:fast_forward: Skipped" >> $GITHUB_OUTPUT
          else
            echo "d1_status=:x: Failed" >> $GITHUB_OUTPUT
          fi

          # Postgres status
          if [ "${{ needs.backup-postgres.result }}" == "success" ]; then
            echo "postgres_status=:white_check_mark: Success" >> $GITHUB_OUTPUT
          elif [ "${{ needs.backup-postgres.result }}" == "skipped" ]; then
            echo "postgres_status=:fast_forward: Skipped" >> $GITHUB_OUTPUT
          else
            echo "postgres_status=:x: Failed" >> $GITHUB_OUTPUT
          fi

          # Cleanup status
          if [ "${{ needs.cleanup.result }}" == "success" ]; then
            echo "cleanup_status=:white_check_mark: Success" >> $GITHUB_OUTPUT
          elif [ "${{ needs.cleanup.result }}" == "skipped" ]; then
            echo "cleanup_status=:fast_forward: Skipped" >> $GITHUB_OUTPUT
          else
            echo "cleanup_status=:warning: Warning" >> $GITHUB_OUTPUT
          fi

          # Check for any warnings
          WARNINGS=""
          if [ "${{ needs.cleanup.result }}" == "failure" ]; then
            WARNINGS="Cleanup job failed - old backups may not be removed"
          fi
          echo "warnings=$WARNINGS" >> $GITHUB_OUTPUT

      - name: Send daily summary notification
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        if: env.SLACK_WEBHOOK_URL != ''
        run: |
          # Build warnings block if any
          WARNINGS_BLOCK=""
          if [ -n "${{ steps.status.outputs.warnings }}" ]; then
            WARNINGS_BLOCK=',{
              "type": "section",
              "text": {
                "type": "mrkdwn",
                "text": ":warning: *Warnings:*\n${{ steps.status.outputs.warnings }}"
              }
            }'
          fi

          curl -X POST -H 'Content-type: application/json' \
            --data '{
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": "Hourly Backup Summary",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*D1 Database:*\n${{ steps.status.outputs.d1_status }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*D1 Size:*\n${{ needs.backup-d1.outputs.backup_size || '\''N/A'\'' }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Auth Postgres:*\n${{ steps.status.outputs.postgres_status }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Auth Size:*\n${{ needs.backup-postgres.outputs.auth_size || '\''N/A'\'' }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Telemetry Postgres:*\n${{ steps.status.outputs.postgres_status }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Telemetry Size:*\n${{ needs.backup-postgres.outputs.telemetry_size || '\''N/A'\'' }}"
                    }
                  ]
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*Cleanup:*\n${{ steps.status.outputs.cleanup_status }}"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Timestamp:*\n'"$(date -u +%Y-%m-%d\ %H:%M\ UTC)"'"
                    }
                  ]
                }'"$WARNINGS_BLOCK"',
                {
                  "type": "context",
                  "elements": [
                    {
                      "type": "mrkdwn",
                      "text": "Repository: ${{ github.repository }} | <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run Details>"
                    }
                  ]
                }
              ]
            }' \
            "$SLACK_WEBHOOK_URL"

  # ===========================================================================
  # Job: Weekly Summary (After B2 Archive)
  # ===========================================================================
  weekly-summary:
    name: Weekly Backup Summary
    runs-on: ubuntu-latest
    needs: [backup-d1, backup-postgres, sync-bundles, archive-to-b2, cleanup]
    # Only run at 3 AM UTC on Sundays (after weekly archive) and if archive succeeded
    if: >
      success() &&
      github.event_name == 'schedule' &&
      github.event.schedule == '0 3 * * 0'

    steps:
      - name: Send weekly summary notification
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        if: env.SLACK_WEBHOOK_URL != ''
        run: |
          curl -X POST -H 'Content-type: application/json' \
            --data '{
              "blocks": [
                {
                  "type": "header",
                  "text": {
                    "type": "plain_text",
                    "text": ":tada: Weekly Backup Complete",
                    "emoji": true
                  }
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "All backup jobs completed successfully for the week!"
                  }
                },
                {
                  "type": "section",
                  "fields": [
                    {
                      "type": "mrkdwn",
                      "text": "*D1 Database:*\n:white_check_mark: Backed up"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Auth Postgres:*\n:white_check_mark: Backed up"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Telemetry Postgres:*\n:white_check_mark: Backed up"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*R2 Bundles:*\n:white_check_mark: Synced (${{ needs.sync-bundles.outputs.object_count || '\''0'\'' }} objects)"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*B2 Archive:*\n:white_check_mark: Updated (${{ needs.archive-to-b2.outputs.total_size || '\''N/A'\'' }})"
                    },
                    {
                      "type": "mrkdwn",
                      "text": "*Cleanup:*\n:white_check_mark: Completed"
                    }
                  ]
                },
                {
                  "type": "divider"
                },
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Archive Week:* ${{ needs.archive-to-b2.outputs.archive_week || '\''N/A'\'' }}\n*Total Archived to B2:* ${{ needs.archive-to-b2.outputs.total_size || '\''N/A'\'' }}"
                  }
                },
                {
                  "type": "context",
                  "elements": [
                    {
                      "type": "mrkdwn",
                      "text": "Repository: ${{ github.repository }} | <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run Details>"
                    }
                  ]
                }
              ]
            }' \
            "$SLACK_WEBHOOK_URL"
